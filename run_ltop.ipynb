{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to run the full LandTrendr Optimization (LTOP) workflow implemented in Python. This tool is intended to be used to select the 'optiumum' version of the LandTrendr change detection algorithm (Kennedy et al., 2010, 2018) for different areas on a landscape. \n",
    "\n",
    "#### Notes\n",
    "- You need to authenticate the run with a specific GEE account. In the import params statement its going to trigger the ee.Authenticate() protocal which will prompt you to a browser page to authenticate using your GEE account. \n",
    "- Subsequent scripts have just the ee.Initialize protocal because you should have already authenicated and this should only happen once. \n",
    "- Right now this is going to import the params from a separate .py file and treat those as imports. It may actually be easier/more straight forward if we're going to stick with a Jupyter Notebook to just put these directly into a cell here in the Jupyter Notebook. \n",
    "- If you import a module at the top and then change something in the exe script you will need to restart or otherwise delete variables because that change won't be reflected otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'generate_LTOP_05' from '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/LTOP_FTV_Py/generate_LTOP_05.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ee \n",
    "import params\n",
    "import importlib\n",
    "importlib.reload(params)\n",
    "import time \n",
    "import pandas as pd \n",
    "import ltop \n",
    "importlib.reload(ltop)\n",
    "import lt_params\n",
    "import run_SNIC_01 as runSNIC\n",
    "importlib.reload(runSNIC)\n",
    "import run_kMeans_02_1 as kmeans_1\n",
    "import run_kMeans_02_2 as kmeans_2\n",
    "import abstract_sampling_03 as ab_img\n",
    "import abstract_imager_04 as run_lt_pts\n",
    "import ltop_lt_paramater_scoring_01 as param_scoring\n",
    "importlib.reload(param_scoring)\n",
    "import generate_LTOP_05 as make_bps\n",
    "importlib.reload(make_bps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['projects/ee-ltop-py/assets/LTOP_full_run/LTOP_SNIC_imagery_cambodia_subset_c2_200_pts_1990',\n",
       " 'projects/ee-ltop-py/assets/LTOP_full_run/LTOP_SNIC_pts_cambodia_subset_c2_200_pts_1990']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ee.data.listAssets({'parent':'projects/ee-ltop-py/assets/LTOP_full_run/'})\n",
    "\n",
    "# sub_test = ['projects/ee-ltop-py/assets/LTOP_testing/synthetic_image_2015',\n",
    "#  'projects/ee-ltop-py/assets/LTOP_testing/synthetic_image_2016',\n",
    "#  'projects/ee-ltop-py/assets/LTOP_testing/synthetic_image_2017',\n",
    "#  'projects/ee-ltop-py/assets/LTOP_testing/synthetic_image_2018',\n",
    "#  'projects/ee-ltop-py/assets/LTOP_testing/synthetic_image_2025']\n",
    "# assets = [a['name'] for a in test['assets'] if 'synthetic' in a['name']]\n",
    "# results = all(e in assets for e in sub_test)\n",
    "# wd = 'projects/ee-ltop-py/assets/LTOP_testing'\n",
    "# folders = [i for i in test['assets'] if i['type'] =='FOLDER']\n",
    "# target = [i for i in folders if i['name'] == wd]\n",
    "\n",
    "# target\n",
    "# if len(target) == 0: \n",
    "#     print('false')\n",
    "# else: \n",
    "#     print('true')\n",
    "[a['name'] for a in test['assets']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is potentially an issue here where running a cell for a large area will result in a problem with the cell hanging. This may be an issue particuarly if you're running this thing in a browser. Its also possible that it will just start a job on the GEE server and then finish running here in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if a task is done we can get the task status and check to see if its done. Define a little function that can be recycled below for that purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_task_status(task_dict): \n",
    "    '''\n",
    "    Input to this function should be a dictionary that is formatted \n",
    "    like the output of task.status()\n",
    "    '''\n",
    "    task_id = task_dict['id']\n",
    "    #for some reason GEE defaults this to a list with a dictioanary as its only item\n",
    "    task_status = ee.data.getTaskStatus(task_id)[0]\n",
    "    \n",
    "    return task_status['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '0.0.2', 'place': 'cambodia_subset', 'startYear': 2015, 'endYear': 2021, 'seedSpacing': 10, 'randomPts': 200, 'imageSource': 'servir', 'assetsRoot': 'projects/ee-ltop-py/assets/', 'assetsChild': 'LTOP_full_run', 'aoi': ee.Geometry({\n",
      "  \"functionInvocationValue\": {\n",
      "    \"functionName\": \"GeometryConstructors.Polygon\",\n",
      "    \"arguments\": {\n",
      "      \"coordinates\": {\n",
      "        \"constantValue\": [\n",
      "          [\n",
      "            [\n",
      "              105.75931157311999,\n",
      "              13.350592925813421\n",
      "            ],\n",
      "            [\n",
      "              105.75931157311999,\n",
      "              13.211590516861936\n",
      "            ],\n",
      "            [\n",
      "              106.03122319421374,\n",
      "              13.211590516861936\n",
      "            ],\n",
      "            [\n",
      "              106.03122319421374,\n",
      "              13.350592925813421\n",
      "            ]\n",
      "          ]\n",
      "        ]\n",
      "      },\n",
      "      \"evenOdd\": {\n",
      "        \"constantValue\": true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}), 'maxClusters': 100, 'minClusters': 0, 'selectedLTparams': <ee.featurecollection.FeatureCollection object at 0x7fd7d8727790>, 'image_source': 'comp', 'param_scoring_inputs': '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/', 'outfile': '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/selected_lt_params/LTOP_Cambodia_zoomed_tc.csv', 'njobs': 8, 'startDate': '11-20', 'endDate': '03-10', 'masked': ['cloud', 'shadow']}\n"
     ]
    }
   ],
   "source": [
    "#run the first SNIC step \n",
    "\n",
    "status1,status2 = runSNIC.generate_snic_outputs(params.params)\n",
    "\n",
    "# status1,status2 = snic.generate_tasks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we run the kmeans algorithm which will automatically grab the snic outputs to do that process. This is not exposed to the user and assumes that you have specified the child and root directories where you want things to go in the params.py file. The following code block will check if the snic process is done and then when it determines that process has concluded it will execute the kmeans step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#note that if you want to just run the kmeans process without having run snic \n",
    "#uncomment the following line of code\n",
    "km_status = kmeans_1.generate_tasks(params.params)\n",
    "while True:\n",
    "    try: \n",
    "        ts_1 = check_task_status(status1) \n",
    "        ts_2 = check_task_status(status2) \n",
    "        \n",
    "        if (ts_1 == 'COMPLETED') & (ts_2 == 'COMPLETED'): \n",
    "            print('The previous task is complete')\n",
    "            km_status = kmeans_1.generate_tasks(params.params)\n",
    "            break\n",
    "        elif (ts_1 == 'FAILED') | (ts_1 == 'CANCELLED'): \n",
    "            print('The first task failed')\n",
    "            break\n",
    "        elif (ts_2 == 'FAILED') | (ts_2 == 'CANCELLED'): \n",
    "            print('The second task failed')\n",
    "            break \n",
    "    except NameError: \n",
    "        print('You did not run the snic step so there is no status to check')\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we just take the kmeans output and do a stratified random sample to get one point for each cluster id in the kmeans output image. Like the previous step, this one will also check to see if the output is done before executing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_pts_status = kmeans_2.generate_tasks(params.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the abstract images. Previously to the Python implementation, these were created from a CSV which was generated in GEE and then pulled down to a local machine. The actual images were constructed in Numpy and then re-uploaded to GEE. This is a pretty inefficient process and therefore we are moving it to a GEE-assets generation type process. This is based on some code that Jack Kilbride wrote to replace the Numpy scheme and is still in testing as of 10/6/2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_imgs_status = ab_img.create_abstract_imgs(params.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to see if we can just make the runParams into something else\n",
    "df = pd.DataFrame.from_records(lt_params.runParams)#.reset_index()#,index=range(len(lt_params.runParams)))\n",
    "\n",
    "# df = df.head(10)\n",
    "df['timeseries'] = None\n",
    "df['timeseries'] = ee.ImageCollection([])\n",
    "output = df.to_dict(orient='records')\n",
    "\n",
    "ls = [x for x in output]\n",
    "ls[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run landtrendr on the abstract image points, using the indices generated in the previous step as inputs. The scripts for the 04 step should be prepped to handle assets as inputs and will expect an imageCollection of abstract images as well as the points that were generated in the previous step that show where the abstract image pixels are located (centroids). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_pt_status = run_lt_pts.run_LT_abstract_imgs(params.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we incorporate the sections that were previously done in Python to accomplish the LT versions scoring. This likely needs to be amended still. Ideally, we wouldn't have to generate the giant csv and move that around for scoring. However, that is a fairly substantial lift to move all of that python code to GEE so for now it will stay as it is but this is a TODO for the future. To accomplish this task we need to: \n",
    "1. download the csv outputs somewhere. Note that this should be done programmatically but its not working correctly with permissions so this is something that we'll need to come back to and streamline. This also depends on what we do with the scoring scripts. \n",
    "2. Run the param selection script\n",
    "3. re-upload the outputs to GEE\n",
    "4. Then we can run the 05 script to generate the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files we are going to process are ['/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/LTOP_servir_comps_revised_abstractImageSample_lt_144params_NBR_c2.csv', '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/LTOP_servir_comps_revised_abstractImageSample_lt_144params_NDVI_c2.csv', '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/LTOP_servir_comps_revised_abstractImageSample_lt_144params_TCG_c2.csv', '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/LTOP_servir_comps_revised_abstractImageSample_lt_144params_TCW_c2.csv', '/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/LTOP_servir_comps_revised_abstractImageSample_lt_144params_B5_c2.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/LTOP_FTV_Py/ltop_lt_paramater_scoring_01.py:253: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/LTOP_FTV_Py/ltop_lt_paramater_scoring_01.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/LTOP_FTV_Py/ltop_lt_paramater_scoring_01.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  these['rankAICc'] = these['AICc'].rank(method='max', ascending=False)\n",
      "/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/LTOP_FTV_Py/ltop_lt_paramater_scoring_01.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  #this is where the weights that were determined from interpreters get applied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating selected LT params file\n"
     ]
    }
   ],
   "source": [
    "#note that the param scoring scripts have been combined into one script \n",
    "input_dir = \"/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/output_04_lt_runs/\"\n",
    "# \tstartYear = 1990 \n",
    "# \tendYear = 2021\n",
    "# \toutfile = \"/vol/v1/general_files/user_files/ben/LTOP_FTV_py_revised/selected_lt_params/selected_tc_lt_params.csv\"\n",
    "njobs = 8\n",
    "\n",
    "outfile = '/vol/v1/proj/LTOP_mekong/csvs/02_param_selection/selected_param_config_gee_implementation/LTOP_Cambodia_troubleshooting_selected_LT_params_tc.csv'\n",
    "param_scoring.generate_selected_params(input_dir,njobs,outfile) \n",
    "\n",
    "# \tmain(input_dir,njobs,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the last (05) step in the LTOP workflow. This step takes in the selected LT versions and it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_vertices_status = make_bps.generate_LTOP_breakpoints(params.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ltop_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "757548dda17506bf7478dcf5c19d54dd277731f0c6e8d9deffb7323e66285ff1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
